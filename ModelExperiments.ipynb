{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from get_training_data import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('train_cases.pkl.gz','rb') as fp:\n",
    "    train_cases = pickle.load(fp)\n",
    "type(train_cases), len(train_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (16384, 16, 16, 128),\n",
       " dtype('float32'),\n",
       " numpy.ndarray,\n",
       " (16384, 1),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_cases[0]\n",
    "train_y = train_cases[1]\n",
    "ntrain = train_X.shape[0]\n",
    "type(train_X), train_X.shape, train_X.dtype, type(train_y), train_y.shape, train_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (4096, 16, 16, 128),\n",
       " dtype('float32'),\n",
       " numpy.ndarray,\n",
       " (4096, 1),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('valid_cases.pkl.gz','rb') as fp:\n",
    "    valid_cases = pickle.load(fp)\n",
    "valid_X = valid_cases[0]\n",
    "valid_y = valid_cases[1]\n",
    "nvalid = valid_X.shape[0]\n",
    "type(valid_X), valid_X.shape, valid_X.dtype, type(valid_y), valid_y.shape, valid_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_training_case():\n",
    "    i = random.randint(0, ntrain-1)\n",
    "    return(train_X[i,:,:,:], train_y[i,:])\n",
    "\n",
    "def choose_validation_case():\n",
    "    i = random.randint(0, nvalid-1)\n",
    "    return(valid_X[i,:,:,:], valid_y[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_CONST = 1e-5\n",
    "CENTER = -2.5\n",
    "SCALE = 5\n",
    "XFORM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xform(a):\n",
    "    if XFORM:\n",
    "        return((np.log(a+ADD_CONST) - CENTER) / SCALE)\n",
    "    else:\n",
    "        return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_from_megabatch(batchsize=32, validation=False):\n",
    "    get_case = choose_validation_case if validation else choose_training_case    \n",
    "    xbatch = np.empty([0, INPUTS_PER_BEAT, NBEATS, NCHANNELS],dtype=np.float32)\n",
    "    ybatch = np.empty([0, 1])\n",
    "    for i in range(batchsize):\n",
    "        case = get_case()\n",
    "        x = np.expand_dims(case[0], axis=0)\n",
    "        xbatch = np.concatenate([xbatch, x], axis=0)\n",
    "        y = np.array(int(case[1])).reshape([1,1])\n",
    "        ybatch = np.concatenate([ybatch, y], axis=0)\n",
    "    return(xform(xbatch), ybatch)\n",
    "\n",
    "def train_gen(batchsize=32):\n",
    "    while True:\n",
    "        yield(make_batch_from_megabatch(batchsize, validation=False))\n",
    "\n",
    "def valid_gen(batchsize=32):\n",
    "    while True:\n",
    "        yield(make_batch_from_megabatch(batchsize, validation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16, 16, 128), (32, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The generators are way too slow: have to fix this\n",
    "for t in train_gen():\n",
    "    break\n",
    "t[0].shape, t[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16, 16, 128), (32, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for v in valid_gen():\n",
    "    break\n",
    "v[0].shape, v[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (INPUTS_PER_BEAT, NBEATS, NCHANNELS)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, 1, input_shape=input_shape)) # 64\n",
    "#model.add(keras.layers.Conv2D(80, 1, input_shape=input_shape))\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0))\n",
    "#model.add(keras.layers.MaxPooling2D(2,1))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.SpatialDropout2D(.1))\n",
    "model.add(keras.layers.Conv2D(32, 1))  # 32\n",
    "#model.add(keras.layers.Conv2D(50, 1))  # 32\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.SpatialDropout2D(.15))\n",
    "#model.add(keras.layers.Conv2D(16, 1, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Conv2D(12, (2,4))) # 16\n",
    "model.add(keras.layers.Conv2D(12, (1,4))) # 16\n",
    "#model.add(keras.layers.Conv2D(16, (2,4))) # 16\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.SpatialDropout2D(.3))\n",
    "#model.add(keras.layers.Conv2D(25, (4,5))) # 29,5\n",
    "model.add(keras.layers.Conv2D(25, (4,6))) # 29,5\n",
    "#model.add(keras.layers.Conv2D(32, (4,6))) # 29,5\n",
    "#model.add(keras.layers.Conv2D(25, 5)) # 29,5\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.SpatialDropout2D(.25))\n",
    "#model.add(keras.layers.Conv2D(3, (2,5))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(4, (2,4))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(3, (2,4))) # 4,3\n",
    "model.add(keras.layers.Conv2D(3, (1,4))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(4, 3)) # 4,3\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.MaxPooling2D(1,2))\n",
    "#model.add(keras.layers.PReLU())\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Conv2D(3, 1))\n",
    "#model.add(keras.layers.LeakyReLU(0))\n",
    "model.add(keras.layers.Flatten())\n",
    "#model.add(keras.layers.Dropout(.2)) # or .2\n",
    "#model.add(keras.layers.Dropout(.25)) # or .2\n",
    "#model.add(keras.layers.Dense(35, activation='relu'))\n",
    "#model.add(keras.layers.Dense(25, activation='relu'))\n",
    "model.add(keras.layers.Dropout(.6)) # .5 or .6\n",
    "#model.add(keras.layers.GaussianDropout(.5))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=.003,decay=2e-4), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_596 (Conv2D)          (None, 16, 16, 64)        8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_596 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_239 (Spati (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_597 (Conv2D)          (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_597 (LeakyReLU)  (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_239 (Bat (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_598 (Conv2D)          (None, 16, 13, 12)        1548      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_598 (LeakyReLU)  (None, 16, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_240 (Spati (None, 16, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_599 (Conv2D)          (None, 13, 8, 25)         7225      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_599 (LeakyReLU)  (None, 13, 8, 25)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_240 (Bat (None, 13, 8, 25)         100       \n",
      "_________________________________________________________________\n",
      "conv2d_600 (Conv2D)          (None, 13, 5, 3)          303       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_600 (LeakyReLU)  (None, 13, 5, 3)          0         \n",
      "_________________________________________________________________\n",
      "flatten_120 (Flatten)        (None, 195)               0         \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 195)               0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 1)                 196       \n",
      "=================================================================\n",
      "Total params: 19,836\n",
      "Trainable params: 19,722\n",
      "Non-trainable params: 114\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "64/64 [==============================] - 12s 184ms/step - loss: 0.6680 - acc: 0.7129 - val_loss: 0.6105 - val_acc: 0.7573\n",
      "Epoch 2/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5965 - acc: 0.7573 - val_loss: 0.7006 - val_acc: 0.7441\n",
      "Epoch 3/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5724 - acc: 0.7739 - val_loss: 0.6291 - val_acc: 0.7622\n",
      "Epoch 4/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5557 - acc: 0.7773 - val_loss: 0.5790 - val_acc: 0.7573\n",
      "Epoch 5/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5396 - acc: 0.7769 - val_loss: 0.5483 - val_acc: 0.7583\n",
      "Epoch 6/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5357 - acc: 0.7612 - val_loss: 0.5211 - val_acc: 0.7695\n",
      "Epoch 7/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5267 - acc: 0.7578 - val_loss: 0.5157 - val_acc: 0.7500\n",
      "Epoch 8/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.5009 - acc: 0.7451 - val_loss: 0.4981 - val_acc: 0.7734\n",
      "Epoch 9/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4529 - acc: 0.7637 - val_loss: 0.4793 - val_acc: 0.7466\n",
      "Epoch 10/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.4277 - acc: 0.7583 - val_loss: 0.4252 - val_acc: 0.7651\n",
      "Epoch 11/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.3903 - acc: 0.7686 - val_loss: 0.4183 - val_acc: 0.7495\n",
      "Epoch 12/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.3423 - acc: 0.7866 - val_loss: 0.3839 - val_acc: 0.7622\n",
      "Epoch 13/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.3552 - acc: 0.7661 - val_loss: 0.3496 - val_acc: 0.7632\n",
      "Epoch 14/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.3389 - acc: 0.8262 - val_loss: 0.3506 - val_acc: 0.8623\n",
      "Epoch 15/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.3120 - acc: 0.8540 - val_loss: 0.3109 - val_acc: 0.8682\n",
      "Epoch 16/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2993 - acc: 0.8643 - val_loss: 0.3361 - val_acc: 0.8701\n",
      "Epoch 17/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2638 - acc: 0.8823 - val_loss: 0.3186 - val_acc: 0.8525\n",
      "Epoch 18/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2803 - acc: 0.8882 - val_loss: 0.2916 - val_acc: 0.8813\n",
      "Epoch 19/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2271 - acc: 0.9048 - val_loss: 0.2585 - val_acc: 0.8945\n",
      "Epoch 20/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2465 - acc: 0.9038 - val_loss: 0.2894 - val_acc: 0.9004\n",
      "Epoch 21/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2452 - acc: 0.8970 - val_loss: 0.2407 - val_acc: 0.9092\n",
      "Epoch 22/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2428 - acc: 0.9048 - val_loss: 0.2805 - val_acc: 0.8940\n",
      "Epoch 23/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2409 - acc: 0.9048 - val_loss: 0.2627 - val_acc: 0.9053\n",
      "Epoch 24/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2319 - acc: 0.9126 - val_loss: 0.2173 - val_acc: 0.9077\n",
      "Epoch 25/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2209 - acc: 0.9062 - val_loss: 0.2869 - val_acc: 0.8813\n",
      "Epoch 26/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2201 - acc: 0.9253 - val_loss: 0.2513 - val_acc: 0.9077\n",
      "Epoch 27/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2298 - acc: 0.9102 - val_loss: 0.2572 - val_acc: 0.8940\n",
      "Epoch 28/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.2186 - acc: 0.9062 - val_loss: 0.2426 - val_acc: 0.9180\n",
      "Epoch 29/60\n",
      "64/64 [==============================] - 2s 25ms/step - loss: 0.1988 - acc: 0.9194 - val_loss: 0.2401 - val_acc: 0.9102\n",
      "Epoch 30/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1993 - acc: 0.9194 - val_loss: 0.2343 - val_acc: 0.9097\n",
      "Epoch 31/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1963 - acc: 0.9214 - val_loss: 0.2483 - val_acc: 0.9058\n",
      "Epoch 32/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1904 - acc: 0.9170 - val_loss: 0.2426 - val_acc: 0.9082\n",
      "Epoch 33/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1939 - acc: 0.9233 - val_loss: 0.2260 - val_acc: 0.9102\n",
      "Epoch 34/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1891 - acc: 0.9331 - val_loss: 0.2623 - val_acc: 0.8877\n",
      "Epoch 35/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1888 - acc: 0.9204 - val_loss: 0.2439 - val_acc: 0.8994\n",
      "Epoch 36/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1871 - acc: 0.9277 - val_loss: 0.2292 - val_acc: 0.9160\n",
      "Epoch 37/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1881 - acc: 0.9272 - val_loss: 0.2334 - val_acc: 0.9082\n",
      "Epoch 38/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1914 - acc: 0.9321 - val_loss: 0.2516 - val_acc: 0.9019\n",
      "Epoch 39/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1954 - acc: 0.9194 - val_loss: 0.2490 - val_acc: 0.9062\n",
      "Epoch 40/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1901 - acc: 0.9233 - val_loss: 0.2224 - val_acc: 0.9146\n",
      "Epoch 41/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1811 - acc: 0.9351 - val_loss: 0.2340 - val_acc: 0.9092\n",
      "Epoch 42/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1842 - acc: 0.9268 - val_loss: 0.2053 - val_acc: 0.9272\n",
      "Epoch 43/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1652 - acc: 0.9336 - val_loss: 0.2142 - val_acc: 0.9160\n",
      "Epoch 44/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1586 - acc: 0.9458 - val_loss: 0.2417 - val_acc: 0.9102\n",
      "Epoch 45/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1788 - acc: 0.9380 - val_loss: 0.2407 - val_acc: 0.9150\n",
      "Epoch 46/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1665 - acc: 0.9326 - val_loss: 0.2107 - val_acc: 0.9209\n",
      "Epoch 47/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1563 - acc: 0.9385 - val_loss: 0.2095 - val_acc: 0.9204\n",
      "Epoch 48/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1790 - acc: 0.9253 - val_loss: 0.2532 - val_acc: 0.9014\n",
      "Epoch 49/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1908 - acc: 0.9253 - val_loss: 0.2246 - val_acc: 0.9121\n",
      "Epoch 50/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1756 - acc: 0.9380 - val_loss: 0.2214 - val_acc: 0.9141\n",
      "Epoch 51/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1610 - acc: 0.9380 - val_loss: 0.2503 - val_acc: 0.9121\n",
      "Epoch 52/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1519 - acc: 0.9419 - val_loss: 0.2601 - val_acc: 0.9023\n",
      "Epoch 53/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1757 - acc: 0.9268 - val_loss: 0.2623 - val_acc: 0.9155\n",
      "Epoch 54/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1617 - acc: 0.9424 - val_loss: 0.2216 - val_acc: 0.9106\n",
      "Epoch 55/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1531 - acc: 0.9409 - val_loss: 0.2124 - val_acc: 0.9282\n",
      "Epoch 56/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1702 - acc: 0.9297 - val_loss: 0.2170 - val_acc: 0.9141\n",
      "Epoch 57/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1684 - acc: 0.9360 - val_loss: 0.2399 - val_acc: 0.9033\n",
      "Epoch 58/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1493 - acc: 0.9409 - val_loss: 0.2008 - val_acc: 0.9272\n",
      "Epoch 59/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1514 - acc: 0.9443 - val_loss: 0.1909 - val_acc: 0.9209\n",
      "Epoch 60/60\n",
      "64/64 [==============================] - 2s 24ms/step - loss: 0.1578 - acc: 0.9414 - val_loss: 0.1926 - val_acc: 0.9185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdde26d3e80>"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid=valid_gen()\n",
    "train=train_gen()\n",
    "model.fit_generator(train, \n",
    "                    validation_data=valid, validation_steps=64,\n",
    "                    epochs=60, \n",
    "                    steps_per_epoch=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoodGollyMissMollyWithJerryLeeAndFats 287\n",
      "Predicted:  0.011747412    Actual:  False \n",
      "\n",
      "ItsRainingMen 55\n",
      "Predicted:  3.1052994e-11    Actual:  False \n",
      "\n",
      "53rdAnd3rdMP 136\n",
      "Predicted:  0.8852352    Actual:  True \n",
      "\n",
      "53rdAnd3rdMP 46\n",
      "Predicted:  2.7495596e-05    Actual:  False \n",
      "\n",
      "SmoothOperator 48\n",
      "Predicted:  0.00070420053    Actual:  False \n",
      "\n",
      "WalkThisWay1 43\n",
      "Predicted:  2.4785813e-06    Actual:  False \n",
      "\n",
      "Kalamazoo 99\n",
      "Predicted:  0.0045318445    Actual:  False \n",
      "\n",
      "IllBeThereForYou 43\n",
      "Predicted:  0.00271275    Actual:  False \n",
      "\n",
      "ChatanoogaChooChoo 58\n",
      "Predicted:  0.14735419    Actual:  False \n",
      "\n",
      "WalkThisWay1 73\n",
      "Predicted:  0.00036416837    Actual:  False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    song, tempo, compatible, clip = get_validation_case()\n",
    "    print( song, tempo )\n",
    "    c = clip_to_tf_input(resample_clip(clip))\n",
    "    x = np.expand_dims(xform(c), axis=0)\n",
    "    p = model.predict(x)[0][0]\n",
    "    print( 'Predicted: ', p, '   Actual: ', compatible, '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "(12,1x4) -> 2282, 2250, 2259, *2247*, **2233**, 2155, 2174, 2493, *2228*, 2386, 2060,\n",
    "            2049, 2008\n",
    "(12,2x3) -> 2083, *2222*, 2807, 2188, **2266**, 2498, *2375*\n",
    "(12,2x5) -> 2297\n",
    "(25,4x5) -> 2170, *2259*, *2357*, **2312**, 2237, 2830, 2228, 2378, 2366\n",
    "(25,4x7) -> 2537, 2282\n",
    "(25,5x6) -> **2372**, *2234*, 2225, 2160, 2567, *2410*, 2482\n",
    "(25,3x6) -> *2259*, 2257, 2361, 2222, 2068, 2212, 2415, 2416, *2305*, 2253, **2262**,\n",
    "            2410, 2487\n",
    "    \n",
    "New baseline from below:\n",
    "(3,1x4) -> *2263*, 2097, 2158, 2106, 2357, 2242, *2248*, 2284, 2484, **2256**, 2572,\n",
    "           2243, 2244, 2359, 2303, 2371, 2247, 2382, 2481, 2236, 2108</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "(3,1x4) -> *2263*, 2097, 2158, 2106, 2357, 2242, *2248*, 2284, 2484, **2256**, 2572,\n",
    "           2243, 2244, 2359, 2303, 2371, 2247, 2382, 2481, 2236, 2108\n",
    "           \n",
    "(3,3x4) -> 2277, 2867\n",
    "(3,2x3) -> 2232, 2254, 2247, 2303, 2356, 2396, 2400, 2193, 2385, 2200, 2084, \n",
    "           *2275*, 2541, 2411, 2539, 2124, 2250, 2371, 2119, **2279**, 2188,\n",
    "           2289, *2280*, \n",
    "           \n",
    "80/50/12/25/3 f -> 2384, 2366, 2168, 2139, *2280*, 2440, **2285**, *2267*, 2429\n",
    "50/20/12/25/3 f -> 2164, 2065, 2497, 2370, 2405, **2381**, 2393\n",
    "15/30/60 d/o -> 2361, 2205, 2083, 2267, 2424, 2342, 2214, 2340, 2143, 2145, **2302**,\n",
    "                2311, *2293*, 2348, *2336*\n",
    "    \n",
    "New baseline from below:\n",
    "*16f to 12f -> 2122, 2283, 2089, 2319, 2251, 2427, 2291, 2353, 2272, 2410, *2276*,\n",
    "               2309, 2161, 2349, 2260, 2229, 2250, 2470, **2275**, 2334, 2266,\n",
    "               *2269*, 2088*\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "*16f to 12f -> 2122, *2283*, 2089, 2319, 2251, 2427, 2291, 2353, *2272*, 2410, **2276**,\n",
    "               2309, 2161, 2349, 2260, 2229, 2250 *\n",
    "    \n",
    "New baseline from below:\n",
    "10//30//60 -> 2284, 2156, 2438, 2451, 2139, 2457, 2281, 2430, 2454, 2328, 2610, \n",
    "              **2295**, 2403, 2095, 2597, 2358, 2155, 2241, *2302*, 2377, 2132,\n",
    "              2290, 2586, 2235, *2292*, 2225, 2213, 2335, 2291\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "below w/o 10 -> 2455, 2336\n",
    "*10//30//60 w 4f on top -> 2306, 2510, 2263, 2279, *2319*, 2163, 2368, *2337*, 2396, \n",
    "                           2167, 2138, 2238, 3505, 2114, 2465, 2359, **2329**, 2581, 2177,\n",
    "                           2415, 2517 *\n",
    "[above w extra b/n bottom -> 2646, 2336]\n",
    "10//30//60 -> 2284, 2156, 2438, 2451, 2139, 2457, 2281, 2430, 2454, 2328, 2610, \n",
    "              **2295**, 2403, 2095, 2597, 2358, 2155, 2241, *2302*, 2377, 2132,\n",
    "              2290, 2586, 2235, *2292*, 2225, 2213\n",
    "/15//25/55 -> 2461, 2321, 2453, 2737\n",
    "//40//50   -> 2555\n",
    "//20/30/60 -> 2789\n",
    "/20//30/60 -> *2394*, 2196, 2450, **2368**, *2359*, 2541, 2327\n",
    "//40//60   -> 2227, *2368*, 2170, 2458, 2274, *2429*, **2419**, 2463, 2497\n",
    "//30//70   -> 2442, 2358\n",
    "//30//50   -> 2509, 2552\n",
    "    \n",
    "baseline: preproc sp d/o replaces b/n for middle layer (d/o=//30//60) -> \n",
    "    2258, 2308, 2236, 2585, 2487, 2599, 2238, *2326*, 2316, 2395, 2484,\n",
    "    **2335**, 2280, *2337*, 2693, 2272, 2523, 2532, 2305, 2562, 2222\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
