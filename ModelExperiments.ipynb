{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from get_training_data import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('train_cases.pkl.gz','rb') as fp:\n",
    "    train_cases = pickle.load(fp)\n",
    "type(train_cases), len(train_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (16384, 16, 16, 128),\n",
       " dtype('float32'),\n",
       " numpy.ndarray,\n",
       " (16384, 1),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_cases[0]\n",
    "train_y = train_cases[1]\n",
    "ntrain = train_X.shape[0]\n",
    "type(train_X), train_X.shape, train_X.dtype, type(train_y), train_y.shape, train_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (4096, 16, 16, 128),\n",
       " dtype('float32'),\n",
       " numpy.ndarray,\n",
       " (4096, 1),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('valid_cases.pkl.gz','rb') as fp:\n",
    "    valid_cases = pickle.load(fp)\n",
    "valid_X = valid_cases[0]\n",
    "valid_y = valid_cases[1]\n",
    "nvalid = valid_X.shape[0]\n",
    "type(valid_X), valid_X.shape, valid_X.dtype, type(valid_y), valid_y.shape, valid_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_training_case():\n",
    "    i = random.randint(0, ntrain-1)\n",
    "    return(train_X[i,:,:,:], train_y[i,:])\n",
    "\n",
    "def choose_validation_case():\n",
    "    i = random.randint(0, nvalid-1)\n",
    "    return(valid_X[i,:,:,:], valid_y[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_from_megabatch(batchsize=32, validation=False):\n",
    "    get_case = choose_validation_case if validation else choose_training_case    \n",
    "    xbatch = np.empty([0, INPUTS_PER_BEAT, NBEATS, NCHANNELS],dtype=np.float32)\n",
    "    ybatch = np.empty([0, 1])\n",
    "    for i in range(batchsize):\n",
    "        case = get_case()\n",
    "        x = np.expand_dims(case[0], axis=0)\n",
    "        xbatch = np.concatenate([xbatch, x], axis=0)\n",
    "        y = np.array(int(case[1])).reshape([1,1])\n",
    "        ybatch = np.concatenate([ybatch, y], axis=0)\n",
    "    return(xbatch, ybatch)\n",
    "\n",
    "def train_gen(batchsize=32):\n",
    "    while True:\n",
    "        yield(make_batch_from_megabatch(batchsize, validation=False))\n",
    "\n",
    "def valid_gen(batchsize=32):\n",
    "    while True:\n",
    "        yield(make_batch_from_megabatch(batchsize, validation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16, 16, 128), (32, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The generators are way too slow: have to fix this\n",
    "for t in train_gen():\n",
    "    break\n",
    "t[0].shape, t[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16, 16, 128), (32, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for v in valid_gen():\n",
    "    break\n",
    "v[0].shape, v[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (INPUTS_PER_BEAT, NBEATS, NCHANNELS)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, 1, input_shape=input_shape)) # 64\n",
    "#model.add(keras.layers.Conv2D(32, 1, input_shape=input_shape))\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(32, 1))  # 32\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Conv2D(16, 1, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(16, (2,4))) # 16\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(25, (4,6))) # 29,5\n",
    "#model.add(keras.layers.Conv2D(25, 5)) # 29,5\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(4, (2,4))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(4, 3)) # 4,3\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.PReLU())\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Conv2D(3, 5))\n",
    "#model.add(keras.layers.LeakyReLU(0))\n",
    "#model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Flatten())\n",
    "#model.add(keras.layers.Dropout(.3)) # or .2\n",
    "#model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dropout(.5)) # .5 or .6\n",
    "#model.add(keras.layers.GaussianDropout(.5))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_341 (Conv2D)          (None, 16, 16, 64)        8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_341 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_273 (Bat (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_342 (Conv2D)          (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_342 (LeakyReLU)  (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_274 (Bat (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_343 (Conv2D)          (None, 15, 13, 16)        4112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_343 (LeakyReLU)  (None, 15, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_275 (Bat (None, 15, 13, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_344 (Conv2D)          (None, 12, 8, 25)         9625      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_344 (LeakyReLU)  (None, 12, 8, 25)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_276 (Bat (None, 12, 8, 25)         100       \n",
      "_________________________________________________________________\n",
      "conv2d_345 (Conv2D)          (None, 11, 5, 4)          804       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_345 (LeakyReLU)  (None, 11, 5, 4)          0         \n",
      "_________________________________________________________________\n",
      "flatten_69 (Flatten)         (None, 220)               0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 220)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 1)                 221       \n",
      "=================================================================\n",
      "Total params: 25,646\n",
      "Trainable params: 25,372\n",
      "Non-trainable params: 274\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "64/64 [==============================] - 8s 129ms/step - loss: 0.6457 - acc: 0.6992 - val_loss: 0.5836 - val_acc: 0.7485\n",
      "Epoch 2/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.5673 - acc: 0.7544 - val_loss: 0.5524 - val_acc: 0.7607\n",
      "Epoch 3/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.5510 - acc: 0.7695 - val_loss: 0.5460 - val_acc: 0.7720\n",
      "Epoch 4/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.5582 - acc: 0.7578 - val_loss: 0.5514 - val_acc: 0.7578\n",
      "Epoch 5/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.5403 - acc: 0.7646 - val_loss: 0.5200 - val_acc: 0.7749\n",
      "Epoch 6/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.5292 - acc: 0.7646 - val_loss: 0.5293 - val_acc: 0.7568\n",
      "Epoch 7/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.5102 - acc: 0.7681 - val_loss: 0.5428 - val_acc: 0.7505\n",
      "Epoch 8/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.4716 - acc: 0.7842 - val_loss: 0.4983 - val_acc: 0.7695\n",
      "Epoch 9/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.4573 - acc: 0.7827 - val_loss: 0.5312 - val_acc: 0.7510\n",
      "Epoch 10/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.4904 - acc: 0.7476 - val_loss: 0.5067 - val_acc: 0.7720\n",
      "Epoch 11/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.4427 - acc: 0.7783 - val_loss: 0.4860 - val_acc: 0.7446\n",
      "Epoch 12/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.4191 - acc: 0.7808 - val_loss: 0.5011 - val_acc: 0.7690\n",
      "Epoch 13/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.4150 - acc: 0.7734 - val_loss: 0.5679 - val_acc: 0.7534\n",
      "Epoch 14/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.4165 - acc: 0.7661 - val_loss: 0.4860 - val_acc: 0.7549\n",
      "Epoch 15/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.4076 - acc: 0.7866 - val_loss: 0.4342 - val_acc: 0.7827\n",
      "Epoch 16/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3600 - acc: 0.8228 - val_loss: 0.4420 - val_acc: 0.7905\n",
      "Epoch 17/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3792 - acc: 0.8091 - val_loss: 0.4344 - val_acc: 0.7876\n",
      "Epoch 18/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3681 - acc: 0.8223 - val_loss: 0.4193 - val_acc: 0.7988\n",
      "Epoch 19/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3405 - acc: 0.8320 - val_loss: 0.4873 - val_acc: 0.7393\n",
      "Epoch 20/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3476 - acc: 0.8389 - val_loss: 0.4137 - val_acc: 0.8154\n",
      "Epoch 21/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3236 - acc: 0.8564 - val_loss: 0.4116 - val_acc: 0.8369\n",
      "Epoch 22/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3545 - acc: 0.8442 - val_loss: 0.4630 - val_acc: 0.8350\n",
      "Epoch 23/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3129 - acc: 0.8604 - val_loss: 0.3463 - val_acc: 0.8423\n",
      "Epoch 24/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.3046 - acc: 0.8677 - val_loss: 0.4269 - val_acc: 0.7979\n",
      "Epoch 25/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2952 - acc: 0.8828 - val_loss: 0.3519 - val_acc: 0.8491\n",
      "Epoch 26/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2854 - acc: 0.8833 - val_loss: 0.3977 - val_acc: 0.8442\n",
      "Epoch 27/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2857 - acc: 0.8862 - val_loss: 0.4715 - val_acc: 0.7329\n",
      "Epoch 28/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2733 - acc: 0.8813 - val_loss: 0.3741 - val_acc: 0.8257\n",
      "Epoch 29/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2602 - acc: 0.8877 - val_loss: 0.3480 - val_acc: 0.8496\n",
      "Epoch 30/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2497 - acc: 0.8984 - val_loss: 0.3611 - val_acc: 0.8315\n",
      "Epoch 31/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2605 - acc: 0.9014 - val_loss: 0.3138 - val_acc: 0.8716\n",
      "Epoch 32/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2307 - acc: 0.9175 - val_loss: 0.3161 - val_acc: 0.8735\n",
      "Epoch 33/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2457 - acc: 0.9053 - val_loss: 0.3480 - val_acc: 0.8589\n",
      "Epoch 34/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2460 - acc: 0.9111 - val_loss: 0.2810 - val_acc: 0.9028\n",
      "Epoch 35/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2396 - acc: 0.9136 - val_loss: 0.3272 - val_acc: 0.8633\n",
      "Epoch 36/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2277 - acc: 0.9160 - val_loss: 0.3319 - val_acc: 0.8735\n",
      "Epoch 37/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2353 - acc: 0.9121 - val_loss: 0.3810 - val_acc: 0.8472\n",
      "Epoch 38/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2126 - acc: 0.9297 - val_loss: 0.3397 - val_acc: 0.8687\n",
      "Epoch 39/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1993 - acc: 0.9253 - val_loss: 0.3058 - val_acc: 0.8618\n",
      "Epoch 40/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2280 - acc: 0.9160 - val_loss: 0.2922 - val_acc: 0.8848\n",
      "Epoch 41/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2188 - acc: 0.9160 - val_loss: 0.2932 - val_acc: 0.8691\n",
      "Epoch 42/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2012 - acc: 0.9385 - val_loss: 0.3281 - val_acc: 0.8662\n",
      "Epoch 43/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2129 - acc: 0.9214 - val_loss: 0.3008 - val_acc: 0.8848\n",
      "Epoch 44/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1950 - acc: 0.9292 - val_loss: 0.3015 - val_acc: 0.8726\n",
      "Epoch 45/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2112 - acc: 0.9160 - val_loss: 0.3163 - val_acc: 0.8604\n",
      "Epoch 46/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1931 - acc: 0.9307 - val_loss: 0.3195 - val_acc: 0.8735\n",
      "Epoch 47/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1933 - acc: 0.9287 - val_loss: 0.2979 - val_acc: 0.8887\n",
      "Epoch 48/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1761 - acc: 0.9331 - val_loss: 0.2930 - val_acc: 0.8774\n",
      "Epoch 49/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1967 - acc: 0.9272 - val_loss: 0.2683 - val_acc: 0.8857\n",
      "Epoch 50/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.2094 - acc: 0.9263 - val_loss: 0.2987 - val_acc: 0.8770\n",
      "Epoch 51/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1890 - acc: 0.9302 - val_loss: 0.2612 - val_acc: 0.8862\n",
      "Epoch 52/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1826 - acc: 0.9355 - val_loss: 0.2658 - val_acc: 0.8848\n",
      "Epoch 53/60\n",
      "64/64 [==============================] - 1s 19ms/step - loss: 0.1787 - acc: 0.9355 - val_loss: 0.2664 - val_acc: 0.8892\n",
      "Epoch 54/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1732 - acc: 0.9346 - val_loss: 0.2593 - val_acc: 0.8950\n",
      "Epoch 55/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1693 - acc: 0.9365 - val_loss: 0.2962 - val_acc: 0.8784\n",
      "Epoch 56/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1580 - acc: 0.9478 - val_loss: 0.2667 - val_acc: 0.8960\n",
      "Epoch 57/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1748 - acc: 0.9390 - val_loss: 0.2801 - val_acc: 0.8638\n",
      "Epoch 58/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1573 - acc: 0.9468 - val_loss: 0.2862 - val_acc: 0.8950\n",
      "Epoch 59/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1704 - acc: 0.9312 - val_loss: 0.2571 - val_acc: 0.8950\n",
      "Epoch 60/60\n",
      "64/64 [==============================] - 1s 18ms/step - loss: 0.1624 - acc: 0.9404 - val_loss: 0.2900 - val_acc: 0.8872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa2d99eaba8>"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid=valid_gen()\n",
    "train=train_gen()\n",
    "model.fit_generator(train, \n",
    "                    validation_data=valid, validation_steps=64,\n",
    "                    epochs=60, \n",
    "                    steps_per_epoch=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EternalFlame 115\n",
      "Predicted:  0.015809983    Actual:  False \n",
      "\n",
      "EternalFlame 115\n",
      "Predicted:  0.020323075    Actual:  False \n",
      "\n",
      "SmoothOperator 48\n",
      "Predicted:  8.087957e-06    Actual:  False \n",
      "\n",
      "ChatanoogaChooChoo 42\n",
      "Predicted:  0.10401919    Actual:  False \n",
      "\n",
      "ChatanoogaChooChoo 235\n",
      "Predicted:  0.6460627    Actual:  False \n",
      "\n",
      "HoldMeTight 272\n",
      "Predicted:  0.7731308    Actual:  True \n",
      "\n",
      "IfIHadAHammer 75\n",
      "Predicted:  0.029689062    Actual:  False \n",
      "\n",
      "InTheHillsOfShiloh 75\n",
      "Predicted:  0.4024067    Actual:  False \n",
      "\n",
      "ChatanoogaChooChoo 75\n",
      "Predicted:  0.00031394788    Actual:  False \n",
      "\n",
      "Kalamazoo 68\n",
      "Predicted:  0.5599123    Actual:  True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    song, tempo, compatible, clip = get_validation_case()\n",
    "    print( song, tempo )\n",
    "    c = clip_to_tf_input(resample_clip(clip))\n",
    "    x = np.expand_dims(c, axis=0)\n",
    "    p = model.predict(x)[0][0]\n",
    "    print( 'Predicted: ', p, '   Actual: ', compatible, '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "from 4f baseline:\n",
    "3x3 conv to 2x4    -> 2809, 3087, 3104, 2887, 2767, 2852, 2763, 3067, 2994, 3153, 3011,\n",
    "                      **2929**, 2777, 2745, 2806, 3070, 3197\n",
    "above + 5x5 to 4x6 -> 3163, 2705, 2931, **2829**, 2915, 3111, 2765, 2791, 3152, 2813, 2797,\n",
    "                      2771, *2836*, 2876, 3039, 2951, 2726, 3144, 2734, 2699, 2732,\n",
    "                      2968, 2755, 3082, 2881, 2723, 2809, 3320, 2659, 2936, *2814*\n",
    "above + 5x5 to 6x4 -> 2871, 3178\n",
    "3x3 conv to 4x2 -> 3297, 3458\n",
    "*2x4, 4x6, 2x4      -> 2728, 2940, 3030, *2821*, 2440, 2911, 2914, 2924, 3134, 2708, 2534,\n",
    "                      2910, 3352, 2870, 2790, **2816**, 2735, 2763, 2767, 2942, 3030,\n",
    "                      2786, 2942, 2772, 3089, 2777, 2854, 2690, 2912, 2659, 2799,\n",
    "                      2770, *2801* *\n",
    "    \n",
    "from 5f baseline:\n",
    "drop bottom b/n -> 3050, 3032, 2667, 3353, 2736, **3132**, 3210, 3199, 3359\n",
    "drop top b/n -> 3266, 2870, 3306, 3039, 2917, **3201**, **3040**, 3304\n",
    "60-25-50-20-5 -> 3237, 3136\n",
    "</code><code>`\n",
    "80-50-20-32-5 -> 2939, **3086**, 2849, 3110, 2990, 3200, 3111\n",
    "50-20-14-25-4 -> 3379\n",
    "80-50-16-25-4 -> 3165, **3079**, 2868, 2994, 3093, 2995, 2951, 3268, 3145\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "with ,5 baseline:\n",
    "activate after b/n -> 3054, 3095, 3618, 3548, **3338**\n",
    "leaky .1 -> 3149\n",
    "prelu -> 3080, 3609, 3620\n",
    "prelu d/o=.6 -> 3734\n",
    "1-1-3-5-3-1 -> 2801, 3374, 3578, 3305, 3015\n",
    "gaussian d/o -> 2911, 3217, 2953, 3097, 3095, 3376, **3212**, 3427, 3706\n",
    "1-1-3-5-5 -> 3215, 3180\n",
    "leaky .2 on bottom 2 layers -> 2725, 3098, 2977, 3094, 2863, 3073, 3206, 3320, 3036, \n",
    "                             **3077**, 3217 SOMETIMES d/o=.6\n",
    "leaky .2 on bottom 2 layers -> 3189, 2989, 3454, 3088, 2928, 3036, **3094**, 3500, 3238,\n",
    "                               3194, 3073, 3066, 3357\n",
    "leaky .4 on bottom 2 layers -> 3248\n",
    "leaky .2/.1 on bottom layers -> 2839, 2832, **3135**, 2908, 3350, 3075, 3161, 3342, 3247\n",
    "1-1-1-5-5 -> 3315, 3229\n",
    "16 filters to 32 -> 3472\n",
    "delete bottom layer -> 3820\n",
    "extra 16 layer -> 3312\n",
    "8 filters to 32 -> **3174**, 2941, 3190\n",
    "8,4 filters to 25,5 -> 2801, 3542, 2648, 3495, 2957, 3255, 2841, 2843, 2863, 2701, 3681,\n",
    "                       3207, 3480, 3262, 2864, 2839, 3056, **3029**, 2928, 3086, 3240,\n",
    "                       2792, 2899, 3097, 2880, 2969, 3049, 3004, 2736, 3621,\n",
    "                       3090, 3305, 2955, 3249, 3193\n",
    "    \n",
    "above w .65 dropout -> 2924, 2897, 3077, 2902, 3279, 3015, 2854, 2893, 3157, 2984, 3127,\n",
    "                        2984, 3130, 3094, 3232, 2779, **3043**, 3188, 2983, 3228, 3294\n",
    "above w .75 dropout -> 2908, 3218, 2805, **3047**, 3333, **3081**\n",
    "8,4 filter to 25, 3 -> 3439\n",
    "8,4 filter to 25, 6 -> 3236, 2872, 3313\n",
    "*8,4 filter to 25, 4 -> 2801, 2983, 3097, 2959, 2899, 2939, 3295, 3111, 3105, 2902, 3140,\n",
    "                        2885, 2806, 3087, 2928, 2993, 2901, 3070, 3115, 3057, **2988**,\n",
    "                        2993, 3547, 3287, 2936, 2953, 2873, 3312, 3089, 2983, 2812*\n",
    "                        \n",
    "                        \n",
    "8,4 f to 29,4, d/o=.6 -> 2937, 2845, 3183, 3478, 3002, 3139, 2927, 2961, 3560, 2921, 3266,\n",
    "                         **3113**, 3163 \n",
    "\n",
    "16,8,4 f to 12,25,4 -> 3170, 2928, 3089, 3339, **3166**\n",
    "</code><code>                        \n",
    "add (3,3) layer above, d/o=.5 -> 3181\n",
    "above w d/o=.65 -> 2962, 3142, 2984, 3086, 2914, **3048**, 3625\n",
    "(3,5) layer instead -> 3330\n",
    "above w .5 d/o -> 3290\n",
    "extra 50 dense layer (30/60 d/o) -> 2954, 3378, 2968, 3393, **3113**\n",
    "above w 20/50 dropout -> 3375\n",
    "\n",
    "used .6 baseline\n",
    "leaky .05 -> 3122, 3245\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "1-1-5-5-5 -> 3107, 3481 (median of last 5)\n",
    "1-1-3-5-3 -> 3052, 3053, 2858, 3295, 3636, 2974, 3057, 2834, 3045, 3742, 3182, \n",
    "             3559, 3112, **3104**, 3148, 4013, 3305, 3015\n",
    "1-1-3-5-3-1 -> 3064, 3129, 3062, 3237\n",
    "1-1-3-5-3-1 d/o=.4 -> 2820, 3224, 3211\n",
    "1-1-3-5-3 d/o=.45 -> 3215\n",
    "1-1-3-5-3 d/o=.7 -> 3408\n",
    "1-1-3-5-3-5 -> 3625\n",
    "*1-1-3-5-3 d/o=.5 -> 3021, 3100, 2908, 3027, 2970, 3188, 3044, 3055, 3021, 3143, 3278, \n",
    "                     3378, 3213, 3058, 2873, 2788, 3144, 2777, 3248, 2975, 3244, \n",
    "                     3219, 3570, 3374, 3118, **3082**, 3039 * \n",
    "1-1-3-5-3-3 d/o=.5 -> 3151, 3149, 3533\n",
    "1-1-3-5-3 add maxpool d/o=.5 -> 3193\n",
    "1-1-3-5-3 add maxpool d/o=.65 -> 4213\n",
    "1-1-1-3-5-3 d/o=.5 -> 3204, \n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
