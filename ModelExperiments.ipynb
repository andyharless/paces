{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from get_training_data import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('train_cases.pkl.gz','rb') as fp:\n",
    "    train_cases = pickle.load(fp)\n",
    "type(train_cases), len(train_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (16384, 16, 16, 128),\n",
       " dtype('float32'),\n",
       " numpy.ndarray,\n",
       " (16384, 1),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_cases[0]\n",
    "train_y = train_cases[1]\n",
    "ntrain = train_X.shape[0]\n",
    "type(train_X), train_X.shape, train_X.dtype, type(train_y), train_y.shape, train_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (4096, 16, 16, 128),\n",
       " dtype('float32'),\n",
       " numpy.ndarray,\n",
       " (4096, 1),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('valid_cases.pkl.gz','rb') as fp:\n",
    "    valid_cases = pickle.load(fp)\n",
    "valid_X = valid_cases[0]\n",
    "valid_y = valid_cases[1]\n",
    "nvalid = valid_X.shape[0]\n",
    "type(valid_X), valid_X.shape, valid_X.dtype, type(valid_y), valid_y.shape, valid_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_training_case():\n",
    "    i = random.randint(0, ntrain-1)\n",
    "    return(train_X[i,:,:,:], train_y[i,:])\n",
    "\n",
    "def choose_validation_case():\n",
    "    i = random.randint(0, nvalid-1)\n",
    "    return(valid_X[i,:,:,:], valid_y[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_CONST = 1e-5\n",
    "CENTER = -2.5\n",
    "SCALE = 5\n",
    "XFORM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xform(a):\n",
    "    if XFORM:\n",
    "        return((np.log(a+ADD_CONST) - CENTER) / SCALE)\n",
    "    else:\n",
    "        return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_from_megabatch(batchsize=32, validation=False):\n",
    "    get_case = choose_validation_case if validation else choose_training_case    \n",
    "    xbatch = np.empty([0, INPUTS_PER_BEAT, NBEATS, NCHANNELS],dtype=np.float32)\n",
    "    ybatch = np.empty([0, 1])\n",
    "    for i in range(batchsize):\n",
    "        case = get_case()\n",
    "        x = np.expand_dims(case[0], axis=0)\n",
    "        xbatch = np.concatenate([xbatch, x], axis=0)\n",
    "        y = np.array(int(case[1])).reshape([1,1])\n",
    "        ybatch = np.concatenate([ybatch, y], axis=0)\n",
    "    return(xform(xbatch), ybatch)\n",
    "\n",
    "def train_gen(batchsize=32):\n",
    "    while True:\n",
    "        yield(make_batch_from_megabatch(batchsize, validation=False))\n",
    "\n",
    "def valid_gen(batchsize=32):\n",
    "    while True:\n",
    "        yield(make_batch_from_megabatch(batchsize, validation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16, 16, 128), (32, 1))"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The generators are way too slow: have to fix this\n",
    "for t in train_gen():\n",
    "    break\n",
    "t[0].shape, t[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16, 16, 128), (32, 1))"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for v in valid_gen():\n",
    "    break\n",
    "v[0].shape, v[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (INPUTS_PER_BEAT, NBEATS, NCHANNELS)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, 1, input_shape=input_shape)) # 64\n",
    "#model.add(keras.layers.Conv2D(50, 1, input_shape=input_shape))\n",
    "#model.add(keras.layers.ELU(1.0))\n",
    "model.add(keras.layers.LeakyReLU(0))\n",
    "#model.add(keras.layers.MaxPooling2D(2,1))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.SpatialDropout2D(.1))\n",
    "model.add(keras.layers.Conv2D(32, 1))  # 32\n",
    "#model.add(keras.layers.Conv2D(32, (2,1)))  # 32\n",
    "#model.add(keras.layers.Conv2D(25, 1))  # 32\n",
    "#model.add(keras.layers.PReLU())\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.LeakyReLU(0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.SpatialDropout2D(.15))\n",
    "#model.add(keras.layers.Conv2D(16, 1, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Conv2D(12, (1,3))) # 16\n",
    "model.add(keras.layers.Conv2D(12, (1,4))) # 16\n",
    "#model.add(keras.layers.Conv2D(14, (1,4))) # 16\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.SpatialDropout2D(.3))\n",
    "#model.add(keras.layers.SpatialDropout2D(.4))\n",
    "#model.add(keras.layers.SpatialDropout2D(.2))\n",
    "#model.add(keras.layers.Conv2D(25, (5,6))) # 29,5\n",
    "model.add(keras.layers.Conv2D(25, (4,6))) # 29,5\n",
    "#model.add(keras.layers.Conv2D(25, (4,7))) # 29,5\n",
    "#model.add(keras.layers.Conv2D(25, 5)) # 29,5\n",
    "#model.add(keras.layers.ELU(0.2))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "#model.add(keras.layers.Conv2D(10, (2,3)))\n",
    "#model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.SpatialDropout2D(.2))\n",
    "\n",
    "#model.add(keras.layers.Conv2D(3, (2,5))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(4, (2,4))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(3, (1,5))) # 4,3\n",
    "model.add(keras.layers.Conv2D(3, (1,4))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(4, 3)) # 4,3\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.MaxPooling2D(1,2))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Conv2D(3, 1))\n",
    "#model.add(keras.layers.LeakyReLU(0))\n",
    "model.add(keras.layers.Flatten())\n",
    "#model.add(keras.layers.Dropout(.9)) # or .2\n",
    "model.add(keras.layers.Dropout(.8)) # or .2\n",
    "#model.add(keras.layers.Dropout(.85)) # or .2\n",
    "#model.add(keras.layers.GaussianDropout(.7)) # or .2\n",
    "#model.add(keras.layers.Dense(25, activation='relu'))\n",
    "#model.add(keras.layers.Dense(24))\n",
    "#model.add(keras.layers.Dense(30))\n",
    "model.add(keras.layers.Dense(20))\n",
    "model.add(keras.layers.PReLU())\n",
    "#model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.Dropout(.2)) # .5 or .6\n",
    "#model.add(keras.layers.Dropout(.15)) # .5 or .6\n",
    "#model.add(keras.layers.GaussianDropout(.1))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "#                             kernel_regularizer=keras.regularizers.l1(2e-6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "#              optimizer=keras.optimizers.Adam(lr=.003,decay=1e-4), \n",
    "              optimizer=keras.optimizers.Adam(lr=.003,decay=2e-4), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_981 (Conv2D)          (None, 16, 16, 64)        8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_981 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_396 (Spati (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_982 (Conv2D)          (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_982 (LeakyReLU)  (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_391 (Bat (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_983 (Conv2D)          (None, 16, 13, 12)        1548      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_983 (LeakyReLU)  (None, 16, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_397 (Spati (None, 16, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_984 (Conv2D)          (None, 13, 8, 25)         7225      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_984 (LeakyReLU)  (None, 13, 8, 25)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_392 (Bat (None, 13, 8, 25)         100       \n",
      "_________________________________________________________________\n",
      "conv2d_985 (Conv2D)          (None, 13, 5, 3)          303       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_985 (LeakyReLU)  (None, 13, 5, 3)          0         \n",
      "_________________________________________________________________\n",
      "flatten_196 (Flatten)        (None, 195)               0         \n",
      "_________________________________________________________________\n",
      "dropout_196 (Dropout)        (None, 195)               0         \n",
      "_________________________________________________________________\n",
      "dense_401 (Dense)            (None, 20)                3920      \n",
      "_________________________________________________________________\n",
      "p_re_lu_196 (PReLU)          (None, 20)                20        \n",
      "_________________________________________________________________\n",
      "dense_402 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 23,601\n",
      "Trainable params: 23,487\n",
      "Non-trainable params: 114\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "64/64 [==============================] - 25s 394ms/step - loss: 0.6965 - acc: 0.7280 - val_loss: 0.5910 - val_acc: 0.7637\n",
      "Epoch 2/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.6003 - acc: 0.7461 - val_loss: 0.5847 - val_acc: 0.7705\n",
      "Epoch 3/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.5636 - acc: 0.7690 - val_loss: 0.5837 - val_acc: 0.7490\n",
      "Epoch 4/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.5486 - acc: 0.7710 - val_loss: 0.5530 - val_acc: 0.7671\n",
      "Epoch 5/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.5438 - acc: 0.7671 - val_loss: 0.5594 - val_acc: 0.7754\n",
      "Epoch 6/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.5313 - acc: 0.7764 - val_loss: 0.5408 - val_acc: 0.7593\n",
      "Epoch 7/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.5427 - acc: 0.7544 - val_loss: 0.5376 - val_acc: 0.7593\n",
      "Epoch 8/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.5132 - acc: 0.7710 - val_loss: 0.5502 - val_acc: 0.7544\n",
      "Epoch 9/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.5084 - acc: 0.7603 - val_loss: 0.5056 - val_acc: 0.7676\n",
      "Epoch 10/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.4855 - acc: 0.7627 - val_loss: 0.5133 - val_acc: 0.7583\n",
      "Epoch 11/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.4498 - acc: 0.7817 - val_loss: 0.4742 - val_acc: 0.7544\n",
      "Epoch 12/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.4506 - acc: 0.7759 - val_loss: 0.4757 - val_acc: 0.7632\n",
      "Epoch 13/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.4357 - acc: 0.7671 - val_loss: 0.4369 - val_acc: 0.7671\n",
      "Epoch 14/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.3974 - acc: 0.7832 - val_loss: 0.4032 - val_acc: 0.7656\n",
      "Epoch 15/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.3685 - acc: 0.7817 - val_loss: 0.3915 - val_acc: 0.7520\n",
      "Epoch 16/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.3395 - acc: 0.7915 - val_loss: 0.3854 - val_acc: 0.8462\n",
      "Epoch 17/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.3235 - acc: 0.8481 - val_loss: 0.3541 - val_acc: 0.8721\n",
      "Epoch 18/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.3252 - acc: 0.8525 - val_loss: 0.3359 - val_acc: 0.8438\n",
      "Epoch 19/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.3058 - acc: 0.8716 - val_loss: 0.2890 - val_acc: 0.8892\n",
      "Epoch 20/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.2575 - acc: 0.8936 - val_loss: 0.2451 - val_acc: 0.9062\n",
      "Epoch 21/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2663 - acc: 0.8916 - val_loss: 0.2818 - val_acc: 0.8818\n",
      "Epoch 22/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2438 - acc: 0.9023 - val_loss: 0.2602 - val_acc: 0.8809\n",
      "Epoch 23/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2365 - acc: 0.8931 - val_loss: 0.2595 - val_acc: 0.8945\n",
      "Epoch 24/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2033 - acc: 0.9199 - val_loss: 0.2187 - val_acc: 0.9033\n",
      "Epoch 25/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.2287 - acc: 0.9058 - val_loss: 0.2308 - val_acc: 0.9048\n",
      "Epoch 26/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2578 - acc: 0.8989 - val_loss: 0.2421 - val_acc: 0.9126\n",
      "Epoch 27/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2163 - acc: 0.9165 - val_loss: 0.2211 - val_acc: 0.9023\n",
      "Epoch 28/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.2141 - acc: 0.9106 - val_loss: 0.2383 - val_acc: 0.9038\n",
      "Epoch 29/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2017 - acc: 0.9141 - val_loss: 0.2585 - val_acc: 0.9058\n",
      "Epoch 30/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2066 - acc: 0.9116 - val_loss: 0.2492 - val_acc: 0.9058\n",
      "Epoch 31/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2331 - acc: 0.9067 - val_loss: 0.2602 - val_acc: 0.9053\n",
      "Epoch 32/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.2076 - acc: 0.9131 - val_loss: 0.2423 - val_acc: 0.9004\n",
      "Epoch 33/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.2202 - acc: 0.9131 - val_loss: 0.2275 - val_acc: 0.9170\n",
      "Epoch 34/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1910 - acc: 0.9243 - val_loss: 0.2443 - val_acc: 0.9019\n",
      "Epoch 35/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2129 - acc: 0.9155 - val_loss: 0.2552 - val_acc: 0.8882\n",
      "Epoch 36/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.1914 - acc: 0.9219 - val_loss: 0.2382 - val_acc: 0.8936\n",
      "Epoch 37/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2162 - acc: 0.9146 - val_loss: 0.2138 - val_acc: 0.9106\n",
      "Epoch 38/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.2163 - acc: 0.9189 - val_loss: 0.2368 - val_acc: 0.9067\n",
      "Epoch 39/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2079 - acc: 0.9170 - val_loss: 0.2283 - val_acc: 0.9106\n",
      "Epoch 40/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.2135 - acc: 0.9131 - val_loss: 0.2429 - val_acc: 0.8945\n",
      "Epoch 41/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1959 - acc: 0.9248 - val_loss: 0.2213 - val_acc: 0.9053\n",
      "Epoch 42/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1950 - acc: 0.9224 - val_loss: 0.2284 - val_acc: 0.9092\n",
      "Epoch 43/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1750 - acc: 0.9336 - val_loss: 0.1939 - val_acc: 0.9224\n",
      "Epoch 44/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1757 - acc: 0.9336 - val_loss: 0.2281 - val_acc: 0.9097\n",
      "Epoch 45/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1970 - acc: 0.9268 - val_loss: 0.2186 - val_acc: 0.9082\n",
      "Epoch 46/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.1908 - acc: 0.9180 - val_loss: 0.2279 - val_acc: 0.9053\n",
      "Epoch 47/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1844 - acc: 0.9326 - val_loss: 0.2211 - val_acc: 0.9111\n",
      "Epoch 48/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1907 - acc: 0.9199 - val_loss: 0.2307 - val_acc: 0.9150\n",
      "Epoch 49/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1891 - acc: 0.9243 - val_loss: 0.2235 - val_acc: 0.9111\n",
      "Epoch 50/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.1977 - acc: 0.9209 - val_loss: 0.1854 - val_acc: 0.9243\n",
      "Epoch 51/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1709 - acc: 0.9292 - val_loss: 0.2227 - val_acc: 0.9058\n",
      "Epoch 52/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1842 - acc: 0.9287 - val_loss: 0.2225 - val_acc: 0.9146\n",
      "Epoch 53/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1919 - acc: 0.9229 - val_loss: 0.2094 - val_acc: 0.9126\n",
      "Epoch 54/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1778 - acc: 0.9287 - val_loss: 0.2189 - val_acc: 0.9150\n",
      "Epoch 55/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.1740 - acc: 0.9380 - val_loss: 0.2280 - val_acc: 0.9033\n",
      "Epoch 56/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.2060 - acc: 0.9219 - val_loss: 0.2148 - val_acc: 0.9155\n",
      "Epoch 57/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1680 - acc: 0.9360 - val_loss: 0.1911 - val_acc: 0.9194\n",
      "Epoch 58/60\n",
      "64/64 [==============================] - 2s 31ms/step - loss: 0.1682 - acc: 0.9331 - val_loss: 0.1906 - val_acc: 0.9185\n",
      "Epoch 59/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1752 - acc: 0.9351 - val_loss: 0.2034 - val_acc: 0.9258\n",
      "Epoch 60/60\n",
      "64/64 [==============================] - 2s 32ms/step - loss: 0.1629 - acc: 0.9365 - val_loss: 0.2480 - val_acc: 0.9004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa338e97e80>"
      ]
     },
     "execution_count": 999,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchsize = 32\n",
    "epochs = 60\n",
    "steps = 2**11/batchsize\n",
    "valid=valid_gen(batchsize)\n",
    "train=train_gen(batchsize)\n",
    "model.fit_generator(train, \n",
    "                    validation_data=valid, validation_steps=steps,\n",
    "                    epochs=60, \n",
    "                    steps_per_epoch=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BothSidesNow 41\n",
      "Predicted:  0.0022004712    Actual:  False \n",
      "\n",
      "SmoothOperator 69\n",
      "Predicted:  9.379174e-10    Actual:  False \n",
      "\n",
      "IThinkWereAloneNow2MP 44\n",
      "Predicted:  6.0262482e-05    Actual:  False \n",
      "\n",
      "ItsRainingMen 185\n",
      "Predicted:  0.00019341834    Actual:  False \n",
      "\n",
      "BloodyMerryMorning 74\n",
      "Predicted:  0.005370181    Actual:  False \n",
      "\n",
      "ChatanoogaChooChoo 76\n",
      "Predicted:  0.09146364    Actual:  False \n",
      "\n",
      "ImLookingThruYou 115\n",
      "Predicted:  8.657952e-05    Actual:  False \n",
      "\n",
      "ImLookingThruYou 172\n",
      "Predicted:  0.9228154    Actual:  True \n",
      "\n",
      "ChatanoogaChooChoo 57\n",
      "Predicted:  0.005663717    Actual:  False \n",
      "\n",
      "Kalamazoo 40\n",
      "Predicted:  0.0001505687    Actual:  False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    song, tempo, compatible, clip = get_validation_case()\n",
    "    print( song, tempo )\n",
    "    c = clip_to_tf_input(resample_clip(clip))\n",
    "    x = np.expand_dims(xform(c), axis=0)\n",
    "    p = model.predict(x)[0][0]\n",
    "    print( 'Predicted: ', p, '   Actual: ', compatible, '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "decay 0001      -> 1975, 2040, 2101, 2125, 2131, 2139, 2151, 2167, 2176, 2182, \n",
    "                   2195, 2195, 2198, 2202, *2204*, **2206**, **2223**, *2239*, 2270, 2287, \n",
    "                   2291, 2296, 2306, 2309, 2318, 2333, 2344, 2368, 2397, 2424, 2431, 2540\n",
    "add conv layer  -> 2396, 2462\n",
    "b/n before relu -> 5191, 5434\n",
    "dense to 30     -> 2273, 2278, 2301\n",
    "dense to 27     -> 2404\n",
    "dense to 25     -> 2242\n",
    "dense to 24     -> 2020, 2029, 2090,  \n",
    "                   2100, 2133, 2140, 2154, 2175, 2195, 2198, 2211\n",
    "                   2223, 2235, 2246, 2274, 2295, 2313, 2323, 2358, 2439, 2458\n",
    "dense to 23     -> 2227, 2326\n",
    "20 d/o middle   -> 2087, 2099, 2102, 2125, 2135, 2184, 2192, \n",
    "                   2195, 2200, *2206*, **2214**, **2221**, *2223*, 2235, 2252, \n",
    "                   2301, 2305, 2310, 2312, 2319, 2347, 2455\n",
    "(32 2x1) below  -> 1987, 2198, *2294*, **2303**, *2337*, 2429, 2680\n",
    "(3, 1x5)        -> 1967, 2018, 2019, 2048, 2053, 2058, 2071, 2073, 2076, 2077, 2081,\n",
    "                   2089, 2089, 2094, 2101, 2103, 2107, 2118, 2119, 2120, 2128, \n",
    "                   2136, 2142, 2147, 2151, 2174, 2180, 2181, 2182, \n",
    "                   2188, 2197, *2200*, **2204**, *2214*, 2215, 2215, 2219\n",
    "                   2226, 2232, 2234, 2235, 2243, 2243, 2244, 2253, 2261, 2264, \n",
    "                   2274, 2276, 2301, 2307, 2309, 2312, 2318, 2341, 2349, 2366, \n",
    "                   2389, 2397, 2407, 2467, 2478, 2520, 2618, 5494,\n",
    "(3, 2x4)        -> 2155, 2161, *2187*, **2305**, *2384*, 2408, 2423\n",
    "(12, 1x3)       -> 2181, 2230, 2267, 2343, 2373\n",
    "(12, 1x5)       -> 2605, 245841 - val_acc: 0.9023\n",
    "(12, 2x4)       -> 2222, 2843\n",
    "(25, 5x6)       -> 2201, 2308, 2324\n",
    "(25, 3x6)       -> 2511, 2438\n",
    "(25, 4x7)       -> 2082, 2278, 2384, 2442\n",
    "(25, 4x5)       -> 2273, 2303, 2344, 5460, \n",
    "    \n",
    "d/o = .85       -> 2049, 2057, 2110, 2122, 2123, 2124, 2132, 2143, 2150, 2165, 2166, 2168, \n",
    "                   2171, 2192, 2193, 2197, *2216*, **2225** **2241**, *2245*, 2257, 2258, 2258,\n",
    "                   2263, 2268, 2271, 2282, 2286, 2306, 2315, 2331, 2332, 2340, 2359, 2393, 5327\n",
    "d/o = .9        -> 2406, 2526\n",
    "d/o = .75       -> 2188, 2121, 2169, \n",
    "                   2192, *2227*, **2248** *2301*, 2327, \n",
    "                   2329, 2329, 2331,\n",
    "40 d/o middle   -> 2087, 2298, 2412, 2436\n",
    "leak .2 @32layr -> 2003, 2107, 2111, 2112, 2126, 2130, 2145, 2147, \n",
    "                   2156, 2161, 2169, 2183, 2196, 2209, *2220*, **2225**, *2239*, 2279, 2287,\n",
    "                   2300, 2311, 2321, 2313, 2348, 2363, 2369, 2386, 2395, 2401, 2509, 2750\n",
    "\n",
    "dense to 16     -> 2108, 2146, 2160, \n",
    "                   2194, 2212, *2212*, **2227**, *2253*, 2296, 2331,\n",
    "                   2375, 2497, 2660\n",
    "25 d/o middle ->   2267, 2291, 2294, 2324, 2399\n",
    "32f to 25 below -> 2078, 2078, 2104, 2227, *2229*, **2236**, *2273*, 2297, 2298, 2405, 2428\n",
    "32f to 50 below -> 2037, 2161, 2220, *2237*, **2245**, *2256*, 2280, 2339, 2471\n",
    "(32 1x2) below  -> 2100, 2140, 2167, 2189, *2233*, **2267**, *2295*, 2317, 2339, 2411, 2426\n",
    "decay 00015     -> 2061, 2185, 2234, **2251**, 2323, 2397, 2469, 2532?\n",
    "    \n",
    "baseline from below:\n",
    ".8/20/prelu/0   -> 2040, 2034, 2052, 2054, 2056, 2058, 2074, 2075, 2078, 2091, 2096, \n",
    "                   2105, 2114, 2120, 2127, 2127, 2132, 2133, 2134, 2139, 2139, 2140. 2140, 2158, \n",
    "                   2162, 2164, 2166, 2166. 2168, 2172, 2174, 2178, 2182, 2188,\n",
    "                   *2190*, **2190**, **2201**, *2203*, 2205, 2209, 2216, 2216, \n",
    "                   2217, 2221, 2233, 2241, 2242, 2242, 2247, 2256, 2269, 2287, 2292, \n",
    "                   2284, 2290, 2291, 2296, 2298, 2300, 2305, 2306, 2317, 2334, 2336, \n",
    "                   2349, 2351, 2355, 2389, 2428, 2480, 2483, 3374,\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "2nd to elu.2 -> 2191, *2230*, 2145, 2262, 2180, 2310, *2200*, 2250, 2168, **2224**, 1961,\n",
    "                2268, 2247                \n",
    "top to elu.3 -> 2370, 2568\n",
    "relu0 to elu -> 2083, 2106, 2310, *2216*, 2105, 2334, 2092, **2288**, *2297*, 2420, 2221\n",
    "gaussian 0.7 -> 2450\n",
    "gaussian d/o -> 2439\n",
    "(32 2x1)     -> **2303**, 1987, 2198, **2294**, 2429, 2680\n",
    "(32 1x2)     -> 2100, 2140, 2167, *2189*, **2233**, *2267*, 2295, 2411, 2426\n",
    "64f to 50    -> 2214, 2315, 2096, **2355**, 2387\n",
    "32f to 50    -> 2161, 2237, 2256, **2245**, 2471\n",
    "32f to 25    -> 2078, **2229**, 2104, *2273*, 2428, *2227*, 2078, 2298, 2405\n",
    "32f to 20    -> 2265, 2081, *2201*, 2102, 2160, *2254*, 2301, **2236**, 2264\n",
    "bottom 80    -> 2041, 2153, 2376, **2263**, *2261*, 2168, 2319, 2438, 2187, *2315*, 2331\n",
    "batchsize=64 -> 2349, 2339\n",
    "batchsize=16 -> 2360, 2315\n",
    "steps/2      -> 2332, 2248 \n",
    "stepsx2      -> 2233, **2205**, 2093, 2186, 2412, 2097, *2198*, 2097, *2217*, 2324, 2166,\n",
    "                2354, 2259\n",
    "    \n",
    "baseline from below:\n",
    "*.8/20/prelu/0   -> *2216*, 2172, 2317, 2247, 2300, 2074, 2091, 2158, 2040, 2296, 2120,\n",
    "                   **2205**, *2190*, 2292, 2190, 2139, 2355, 2188, 2241, 2164, 2305,\n",
    "                   2166, 2291, 2336, 2174, 2162, 2217, 2351, 2078, 2349, 2168,\n",
    "                   2287, 3374\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "below w l2=1e-6 -> 2219, ?\n",
    "maybe that or l1-> 2068, 2170, 2165, 2136, 2269, 2213, 2187, 2363, 2180, 2583,\n",
    "                   2249, 2189, 2268, 2274, 2110, 2153\n",
    "below w l2=5e-6 -> 2521\n",
    "below w l2=2e-5 -> 2258, 2279 \n",
    "    \n",
    "*.8/20/prelu/0   -> 2216, 2172, 2317, 2247, 2300, 2074, 2091, 2158, 2040, 2296, 2120,\n",
    "                   *2205*, **2190**, 2292, *2190*, 2139, 2355, 2188, 2241, 2164, 2305,\n",
    "                   2166, 2291,  *\n",
    "                   \n",
    "above w l1=1e-5 -> 2232\n",
    "above w l1=2e-6 -> *2222*, 2334, 2050, 2065, **2244**, 2325, 2136, 2091, 2364, 2201, 2281,\n",
    "                   *2244*, 2384\n",
    " \n",
    "\n",
    "try again, from below:\n",
    ".7/20/prelu/0   -> *2303*, 2232, 2334, 2086, 2179, 2319, 2127, 2372, 2207, *2243*, **2270**,\n",
    "                   2185, 2353, 2580, 2218, **2244**, 2341, 2331\n",
    "\n",
    ".7/20/prelu/.15 -> 2141, 2321, 2381, 2268, 2386\n",
    ".7/20/prelu/.2  -> 2181, 2140, 2285, 2173, *2257*, **2256**, 2212, *2243*, 2321, 2492, 2306\n",
    "    \n",
    "back to relu, from below:\n",
    "dense .7/20/0   -> 2255, 2191, 2404, 2185, 2115, 2131, 2276, 2389, 2142, 2165, 2100,\n",
    "                   2386, 2352, 2209, 2142, **2228**, 2470, 2267, 2423, 2218, 2487,\n",
    "                   2290, 2323, *2223*, 2160, *2237*, 2433, 2103, 2221, 2287, 2244,\n",
    "                   2364, 2098, 2100, 2120\n",
    "                  \n",
    "    \n",
    "14f mid layer   -> 2213, 2308, 2504, 2351\n",
    "10f mid layer   -> 2392, 2351\n",
    "10f mid layr 2x4-> 2076, 2398, 2370, 2420\n",
    "decay 0001      -> *2246*, 2288, 2087, 2096, 2342, 2155, 2149, 2057, 2163, **2246**, 2417,\n",
    "                   *2273*, 2602  \n",
    "decay 0003      -> 2403, 2356\n",
    "decay 0004      -> 2156, **2278**, *2197*, 2356, 2372, 2140, *2333*\n",
    "    \n",
    "baseline from below:\n",
    ".7/20/prelu/.1 -> 2173, 2176, 2267, 2138, 2216, 2408, 2121, 2424, 2166, 2282, 2230,\n",
    "                   2274, 2227, 2362, 2157, 2274, 2147, 2189, 2413, 2398, 2193,\n",
    "                   2276, 2317, 2331, 2117, 2234, 2217, **2247**, 2317, 2259, *2241*,\n",
    "                   *2255*, 2366\n",
    "                   \n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "below g/d/o @.7 -> 2259, 2289, 2288, 2262\n",
    "*.7/20/prelu/.1 -> 2173, 2176, 2267, 2138, *2216*, 2408, 2121, 2424, 2166, 2282, *2230*,\n",
    "                   2274, **2227**, 2362, 2157, 2274, 2147 *\n",
    "above g/d/o @.1 -> 2334, 2229, 2355, \n",
    "dense .7/20/.1  -> 2400, 2156, 2296, 2416\n",
    ".7/20/prelu/0   -> 2303, 2232, 2334, \n",
    "dense .8/20/0   -> 2360, 2159, 2384, **2291**, 2288\n",
    "dense .6/20/0   -> 2474, 2352\n",
    "dense .7/25/0   -> 2363, 2266, 2284\n",
    "dense .7/15/0   -> 2318, 2234, 2333, 2334\n",
    "dense .7/20/0   -> 2255, 2191, 2404, 2185, 2115, 2131, 2276, 2389, 2142, 2165, 2100,\n",
    "                  2386, 2352, 2209, 2142, **2228**, 2470, 2267, 2423, 2218, 2487,\n",
    "                  2290, 2323, *2223*, 2160, *2237*, 2433, 2103, 2221\n",
    "                  \n",
    "dense .6/15/.1  -> 2044, 2213, *2268*, 2179, *2233*, 2334, 2422, **2256** 2334\n",
    "dense .5/12/.3  -> 2163, 2271, 2353, 2325\n",
    "dense .4/10/.6  -> 2731\n",
    "    \n",
    "baseline from before:\n",
    "(12,1x4) -> 2282, *2250*, 2259, **2247**, 2233, 2155, 2174, 2493, 2228, 2386, 2060,\n",
    "            2049, 2008, 2621, *2237*, 2359, 2212, 2354, 2273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
