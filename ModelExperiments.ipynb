{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from get_training_data import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('train_cases.pkl.gz','rb') as fp:\n",
    "    train_cases = pickle.load(fp)\n",
    "type(train_cases), len(train_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (16384, 16, 16, 128),\n",
       " dtype('float32'),\n",
       " numpy.ndarray,\n",
       " (16384, 1),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = train_cases[0]\n",
    "train_y = train_cases[1]\n",
    "ntrain = train_X.shape[0]\n",
    "type(train_X), train_X.shape, train_X.dtype, type(train_y), train_y.shape, train_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " (4096, 16, 16, 128),\n",
       " dtype('float32'),\n",
       " numpy.ndarray,\n",
       " (4096, 1),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gzip.open('valid_cases.pkl.gz','rb') as fp:\n",
    "    valid_cases = pickle.load(fp)\n",
    "valid_X = valid_cases[0]\n",
    "valid_y = valid_cases[1]\n",
    "nvalid = valid_X.shape[0]\n",
    "type(valid_X), valid_X.shape, valid_X.dtype, type(valid_y), valid_y.shape, valid_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_training_case():\n",
    "    i = random.randint(0, ntrain-1)\n",
    "    return(train_X[i,:,:,:], train_y[i,:])\n",
    "\n",
    "def choose_validation_case():\n",
    "    i = random.randint(0, nvalid-1)\n",
    "    return(valid_X[i,:,:,:], valid_y[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_CONST = 1e-5\n",
    "CENTER = -2.5\n",
    "SCALE = 5\n",
    "XFORM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xform(a):\n",
    "    if XFORM:\n",
    "        return((np.log(a+ADD_CONST) - CENTER) / SCALE)\n",
    "    else:\n",
    "        return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_from_megabatch(batchsize=32, validation=False):\n",
    "    get_case = choose_validation_case if validation else choose_training_case    \n",
    "    xbatch = np.empty([0, INPUTS_PER_BEAT, NBEATS, NCHANNELS],dtype=np.float32)\n",
    "    ybatch = np.empty([0, 1])\n",
    "    for i in range(batchsize):\n",
    "        case = get_case()\n",
    "        x = np.expand_dims(case[0], axis=0)\n",
    "        xbatch = np.concatenate([xbatch, x], axis=0)\n",
    "        y = np.array(int(case[1])).reshape([1,1])\n",
    "        ybatch = np.concatenate([ybatch, y], axis=0)\n",
    "    return(xform(xbatch), ybatch)\n",
    "\n",
    "def train_gen(batchsize=32):\n",
    "    while True:\n",
    "        yield(make_batch_from_megabatch(batchsize, validation=False))\n",
    "\n",
    "def valid_gen(batchsize=32):\n",
    "    while True:\n",
    "        yield(make_batch_from_megabatch(batchsize, validation=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16, 16, 128), (32, 1))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The generators are way too slow: have to fix this\n",
    "for t in train_gen():\n",
    "    break\n",
    "t[0].shape, t[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16, 16, 128), (32, 1))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for v in valid_gen():\n",
    "    break\n",
    "v[0].shape, v[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (INPUTS_PER_BEAT, NBEATS, NCHANNELS)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, 1, input_shape=input_shape)) # 64\n",
    "#model.add(keras.layers.Conv2D(50, 1, input_shape=input_shape))\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0))\n",
    "#model.add(keras.layers.MaxPooling2D(2,1))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.SpatialDropout2D(.1))\n",
    "model.add(keras.layers.Conv2D(32, 1))  # 32\n",
    "#model.add(keras.layers.Conv2D(20, 1))  # 32\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.SpatialDropout2D(.2))\n",
    "#model.add(keras.layers.Conv2D(16, 1, activation='relu'))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Conv2D(40, (2,4))) # 16\n",
    "model.add(keras.layers.Conv2D(16, (2,4))) # 16\n",
    "#model.add(keras.layers.Conv2D(16, (2,6))) # 16\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.SpatialDropout2D(.3))\n",
    "model.add(keras.layers.Conv2D(25, (4,6))) # 29,5\n",
    "#model.add(keras.layers.Conv2D(32, (4,6))) # 29,5\n",
    "#model.add(keras.layers.Conv2D(25, 5)) # 29,5\n",
    "#model.add(keras.layers.PReLU())\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.SpatialDropout2D(.4))\n",
    "#model.add(keras.layers.Conv2D(3, (2,5))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(4, (2,4))) # 4,3\n",
    "model.add(keras.layers.Conv2D(3, (2,4))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(3, (2,3))) # 4,3\n",
    "#model.add(keras.layers.Conv2D(4, 3)) # 4,3\n",
    "model.add(keras.layers.LeakyReLU(0.0))\n",
    "#model.add(keras.layers.MaxPooling2D(1,2))\n",
    "#model.add(keras.layers.PReLU())\n",
    "#model.add(keras.layers.BatchNormalization())\n",
    "#model.add(keras.layers.Conv2D(3, 1))\n",
    "#model.add(keras.layers.LeakyReLU(0))\n",
    "model.add(keras.layers.Flatten())\n",
    "#model.add(keras.layers.Dropout(.2)) # or .2\n",
    "#model.add(keras.layers.Dropout(.25)) # or .2\n",
    "#model.add(keras.layers.Dense(35, activation='relu'))\n",
    "#model.add(keras.layers.Dense(25, activation='relu'))\n",
    "model.add(keras.layers.Dropout(.6)) # .5 or .6\n",
    "#model.add(keras.layers.GaussianDropout(.5))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=.003,decay=2e-4), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_101 (Conv2D)          (None, 16, 16, 64)        8256      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_101 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_102 (Conv2D)          (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_102 (LeakyReLU)  (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_103 (Conv2D)          (None, 15, 13, 16)        4112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_103 (LeakyReLU)  (None, 15, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_26 (Spatia (None, 15, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_104 (Conv2D)          (None, 12, 8, 25)         9625      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_104 (LeakyReLU)  (None, 12, 8, 25)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 12, 8, 25)         100       \n",
      "_________________________________________________________________\n",
      "conv2d_105 (Conv2D)          (None, 11, 5, 3)          603       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_105 (LeakyReLU)  (None, 11, 5, 3)          0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 165)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 165)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 166       \n",
      "=================================================================\n",
      "Total params: 25,326\n",
      "Trainable params: 25,084\n",
      "Non-trainable params: 242\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "64/64 [==============================] - 3s 48ms/step - loss: 0.7021 - acc: 0.6777 - val_loss: 0.5722 - val_acc: 0.7651\n",
      "Epoch 2/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.5961 - acc: 0.7524 - val_loss: 0.5843 - val_acc: 0.7573\n",
      "Epoch 3/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.5795 - acc: 0.7632 - val_loss: 0.5614 - val_acc: 0.7598\n",
      "Epoch 4/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.5781 - acc: 0.7666 - val_loss: 0.5626 - val_acc: 0.7681\n",
      "Epoch 5/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.5633 - acc: 0.7710 - val_loss: 0.5759 - val_acc: 0.7646\n",
      "Epoch 6/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.5740 - acc: 0.7500 - val_loss: 0.5385 - val_acc: 0.7778\n",
      "Epoch 7/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.5298 - acc: 0.7729 - val_loss: 0.5300 - val_acc: 0.7598\n",
      "Epoch 8/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.5177 - acc: 0.7627 - val_loss: 0.5392 - val_acc: 0.7441\n",
      "Epoch 9/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.4645 - acc: 0.7827 - val_loss: 0.5443 - val_acc: 0.7725\n",
      "Epoch 10/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.4673 - acc: 0.7607 - val_loss: 0.4756 - val_acc: 0.7539\n",
      "Epoch 11/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.4208 - acc: 0.7710 - val_loss: 0.5125 - val_acc: 0.7661\n",
      "Epoch 12/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.3873 - acc: 0.7710 - val_loss: 0.4074 - val_acc: 0.7656\n",
      "Epoch 13/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.3730 - acc: 0.7729 - val_loss: 0.3898 - val_acc: 0.7505\n",
      "Epoch 14/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.3576 - acc: 0.7686 - val_loss: 0.3552 - val_acc: 0.7607\n",
      "Epoch 15/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.3374 - acc: 0.8169 - val_loss: 0.4504 - val_acc: 0.6904\n",
      "Epoch 16/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.3154 - acc: 0.8545 - val_loss: 0.3060 - val_acc: 0.8716\n",
      "Epoch 17/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.2834 - acc: 0.8770 - val_loss: 0.2931 - val_acc: 0.8735\n",
      "Epoch 18/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2807 - acc: 0.8809 - val_loss: 0.3066 - val_acc: 0.8740\n",
      "Epoch 19/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2759 - acc: 0.9033 - val_loss: 0.2852 - val_acc: 0.8818\n",
      "Epoch 20/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2620 - acc: 0.8867 - val_loss: 0.3166 - val_acc: 0.8472\n",
      "Epoch 21/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2767 - acc: 0.8887 - val_loss: 0.3135 - val_acc: 0.8770\n",
      "Epoch 22/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2535 - acc: 0.8984 - val_loss: 0.2660 - val_acc: 0.9009\n",
      "Epoch 23/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.2450 - acc: 0.9004 - val_loss: 0.2944 - val_acc: 0.8911\n",
      "Epoch 24/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2356 - acc: 0.9058 - val_loss: 0.3073 - val_acc: 0.8550\n",
      "Epoch 25/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2422 - acc: 0.9033 - val_loss: 0.3359 - val_acc: 0.8428\n",
      "Epoch 26/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2273 - acc: 0.9126 - val_loss: 0.2544 - val_acc: 0.9082\n",
      "Epoch 27/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.2375 - acc: 0.9160 - val_loss: 0.2420 - val_acc: 0.8848\n",
      "Epoch 28/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.2278 - acc: 0.9209 - val_loss: 0.2778 - val_acc: 0.8838\n",
      "Epoch 29/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.2133 - acc: 0.9312 - val_loss: 0.2539 - val_acc: 0.9082\n",
      "Epoch 30/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2236 - acc: 0.9136 - val_loss: 0.2793 - val_acc: 0.8853\n",
      "Epoch 31/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.2118 - acc: 0.9170 - val_loss: 0.2461 - val_acc: 0.8965\n",
      "Epoch 32/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1781 - acc: 0.9351 - val_loss: 0.2227 - val_acc: 0.9043\n",
      "Epoch 33/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1880 - acc: 0.9302 - val_loss: 0.2392 - val_acc: 0.8999\n",
      "Epoch 34/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1933 - acc: 0.9287 - val_loss: 0.2740 - val_acc: 0.8711\n",
      "Epoch 35/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.2169 - acc: 0.9185 - val_loss: 0.2947 - val_acc: 0.8569\n",
      "Epoch 36/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1836 - acc: 0.9365 - val_loss: 0.2302 - val_acc: 0.9014\n",
      "Epoch 37/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.1931 - acc: 0.9263 - val_loss: 0.2690 - val_acc: 0.8916\n",
      "Epoch 38/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1881 - acc: 0.9219 - val_loss: 0.2686 - val_acc: 0.9028\n",
      "Epoch 39/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1685 - acc: 0.9385 - val_loss: 0.2423 - val_acc: 0.8960\n",
      "Epoch 40/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1689 - acc: 0.9355 - val_loss: 0.2095 - val_acc: 0.9165\n",
      "Epoch 41/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1762 - acc: 0.9331 - val_loss: 0.2349 - val_acc: 0.9067\n",
      "Epoch 42/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1643 - acc: 0.9399 - val_loss: 0.2417 - val_acc: 0.8975\n",
      "Epoch 43/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1656 - acc: 0.9375 - val_loss: 0.2699 - val_acc: 0.8984\n",
      "Epoch 44/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1676 - acc: 0.9395 - val_loss: 0.2213 - val_acc: 0.9111\n",
      "Epoch 45/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.1746 - acc: 0.9346 - val_loss: 0.2206 - val_acc: 0.9131\n",
      "Epoch 46/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1654 - acc: 0.9429 - val_loss: 0.2113 - val_acc: 0.9106\n",
      "Epoch 47/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1521 - acc: 0.9434 - val_loss: 0.2358 - val_acc: 0.9033\n",
      "Epoch 48/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.1575 - acc: 0.9341 - val_loss: 0.2469 - val_acc: 0.9004\n",
      "Epoch 49/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.1548 - acc: 0.9458 - val_loss: 0.2391 - val_acc: 0.9019\n",
      "Epoch 50/60\n",
      "64/64 [==============================] - 1s 21ms/step - loss: 0.1518 - acc: 0.9419 - val_loss: 0.2212 - val_acc: 0.9043\n",
      "Epoch 51/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1568 - acc: 0.9424 - val_loss: 0.2125 - val_acc: 0.9048\n",
      "Epoch 52/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1774 - acc: 0.9341 - val_loss: 0.2350 - val_acc: 0.8926\n",
      "Epoch 53/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1499 - acc: 0.9434 - val_loss: 0.2496 - val_acc: 0.9053\n",
      "Epoch 54/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1414 - acc: 0.9497 - val_loss: 0.2227 - val_acc: 0.9160\n",
      "Epoch 55/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1601 - acc: 0.9385 - val_loss: 0.2361 - val_acc: 0.9067\n",
      "Epoch 56/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1329 - acc: 0.9497 - val_loss: 0.2343 - val_acc: 0.9014\n",
      "Epoch 57/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1352 - acc: 0.9517 - val_loss: 0.2180 - val_acc: 0.9268\n",
      "Epoch 58/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1365 - acc: 0.9468 - val_loss: 0.2168 - val_acc: 0.9204\n",
      "Epoch 59/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1500 - acc: 0.9453 - val_loss: 0.2326 - val_acc: 0.9097\n",
      "Epoch 60/60\n",
      "64/64 [==============================] - 1s 20ms/step - loss: 0.1326 - acc: 0.9482 - val_loss: 0.2461 - val_acc: 0.8975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b1628ff60>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid=valid_gen()\n",
    "train=train_gen()\n",
    "model.fit_generator(train, \n",
    "                    validation_data=valid, validation_steps=64,\n",
    "                    epochs=60, \n",
    "                    steps_per_epoch=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigJimMP 82\n",
      "Predicted:  0.8774294    Actual:  True \n",
      "\n",
      "BigJimMP 167\n",
      "Predicted:  0.8721071    Actual:  True \n",
      "\n",
      "BloodyMerryMorning 73\n",
      "Predicted:  0.0028755136    Actual:  False \n",
      "\n",
      "HangingOnTheTelephone 101\n",
      "Predicted:  0.0010437892    Actual:  False \n",
      "\n",
      "BigJimMP 161\n",
      "Predicted:  0.8865248    Actual:  True \n",
      "\n",
      "ImLookingThruYou 43\n",
      "Predicted:  0.03490556    Actual:  False \n",
      "\n",
      "WalkThisWay1 49\n",
      "Predicted:  0.001167191    Actual:  False \n",
      "\n",
      "09 Suddenly I See 44\n",
      "Predicted:  0.00021952346    Actual:  False \n",
      "\n",
      "IThinkWereAloneNow2MP 53\n",
      "Predicted:  7.9605576e-05    Actual:  False \n",
      "\n",
      "WalkThisWay1 48\n",
      "Predicted:  0.00061431504    Actual:  False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    song, tempo, compatible, clip = get_validation_case()\n",
    "    print( song, tempo )\n",
    "    c = clip_to_tf_input(resample_clip(clip))\n",
    "    x = np.expand_dims(xform(c), axis=0)\n",
    "    p = model.predict(x)[0][0]\n",
    "    print( 'Predicted: ', p, '   Actual: ', compatible, '\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "preproc sp d/o replaces b/n for middle layer (d/o=30/60) -> 2258, **2308**, 2236, 2585, 2487, \n",
    "    2599, 2238, **2326**\n",
    "above w d/o=30/50 -> 2509\n",
    "30/70 -> 2442\n",
    "preproc sp d/o no b/n -> 2673, 5457\n",
    "preproc, d/o=.6 -> 3027\n",
    "preproc, d/o=.9 -> 2952\n",
    "preproc, d/o=.7 -> 2951\n",
    "with preproc -> 2936, 3173, 2644\n",
    "baseline (from below):\n",
    "*only 3f on last conv -> **2693**, 2891, 2753, 2821, 2613, 2613, *2672*, 2657, 2913, \n",
    "                         2588, 2878, 2883, 2580, 2597, 2523, **2703**, 2619, 2906, 2898,\n",
    "                         *2745*, 3123, 2536 *\n",
    "</code>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "last conv=(3,2x3) -> *2756*, 2559, 2826, *2715*, 2581, 2775, 2669, 2922, **2728**, \n",
    "                     2509, 2655, 2839, 2413, 2768, 2702, 2890, 3013\n",
    "last conv=(3,3x4) -> 2823, 2784, 2746, 2921\n",
    "last conv=(3,2x5) -> 2483, 3242, 2953\n",
    "64-32-64-25-4 -> 3010\n",
    "2x6, 4x6, 2x4 -> 3085\n",
    "50-20-40-25-4 -> 2686, 2796, 3010, 3022\n",
    "50-20-10-25-4 -> 2842, 2949\n",
    "80-40-20-32-4 -> 3008\n",
    "*only 3f on last conv -> *2693*, 2891, 2753, 2821, 2613, 2613, 2672, 2657, 2913, \n",
    "                         2588, 2878, 2883, 2580, 2597, 2523, **2703**, 2619, 2906, 2898,\n",
    "                         *2745*, 3123 *\n",
    "5f on last conv + (3, 1x1) -> 3455\n",
    "add conv(2, 1x1) on top -> 2613, 2660, 3053, 2754\n",
    "maxpool(1,2) at the top -> 2765, 2963\n",
    "extra dense layer (35 d/o=.25) -> 3111\n",
    "   \n",
    "2x4,4x6,2x4,lr=003,decay=0002 baseline, from below:\n",
    "2531, 2818-, 2422, 2825, 2613, 2576, 2871, 2619, 3204, 2818+, *2785*,\n",
    "3081, 3235, 2715, 2605, *2761*, **2771**\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>from 5f baseline:\n",
    "lr=.004,decay=2e-4 -> 2659, 2679, 2934\n",
    "lr=.002,decay=2e-4 -> 2932\n",
    "lr=.003,decay=1e-4 -> 2835\n",
    "lr=.003,decay=3e-4 -> 2968\n",
    "lr=.002,decay=3e-4 -> 2879, 2977\n",
    "lr=.004,decay=1e-4 -> 3127\n",
    "lr=.004,decay=5e-4 -> 2696, 3148, 2900\n",
    "*lr=.003,decay=2e-4 -> 2531, 2818, 2422, 2825, **2613**, 2576, 2871 *\n",
    "lr=.003,decay=3e-5 -> 3129\n",
    "extra dense layer (25 d/o=.3) -> 2786, **2873**, 3130, 3012, 2744\n",
    "extra dense layer (50 d/o=.2) -> 3202, 2823, **2833**, 2776, 3153\n",
    "change 32 conv to (2,1) -> 2893, 3380\n",
    "add maxpool(2,1) on bottom -> 3599\n",
    "add maxpool(2,1) on top -> 2857, 2977, 3170\n",
    "add 2, 2x3 on top -> 3197, 3070\n",
    "    \n",
    "from 5f baseline:\n",
    "drop bottom b/n -> 3050, 3032, 2667, 3353, 2736, 3132, 3210, 3199, 3359\n",
    "drop top b/n -> 3266, 2870, 3306, 3039, 2917, 3201, 3040, 3304\n",
    "60-25-50-20-5 -> 3237, 3136, 2531, 2818, 2422, 2825, 2613, 2576, 2871\n",
    "\n",
    "80-50-20-32-5 -> 2939, 3086, 2849, 3110, 2990, 3200, 3111\n",
    "50-20-14-25-4 -> 3379\n",
    "80-50-16-25-4 -> 3165, 3079, 2868, 2994, 3093, 2995, 2951, 3268, 3145\n",
    "\n",
    "add 2, 1x1 on top -> 3049, 2936, 2849\n",
    "2x4, 4x7, 2x4 -> 2942, 2910\n",
    "2x4, 4x5, 2x4 -> 3072, 3360from 5f baseline:\n",
    "drop bottom b/n -> 3050, 3032, 2667, 3353, 2736, 3132, 3210, 3199, 3359\n",
    "drop top b/n -> 3266, 2870, 3306, 3039, 2917, 3201, 3040, 3304\n",
    "60-25-50-20-5 -> 3237, 3136\n",
    "\n",
    "2x4, 4x6, 3x4 -> 2886, 2696, 2987, 3009, 2789, 3427, **2979**\n",
    "2x4, 4x6, 2x3 -> 2613, 3383, 2632, 3046, *2889*, **2940**, 2666, 3039, *3019*\n",
    "above w d/o=.65 -> 3238\n",
    "2x4, 4x6, 2x6 -> 2849, **3003**, 2588, 3200, 3008\n",
    "2x4, 4x8, 2x4 -> 2712, **2959**, 3199, 2987, 2934\n",
    "2x6, 4x6, 2x4 -> 2893, *2861*, **2868**, 3101, *2821*, 2581, 3021, 3155, *2880*\n",
    "    \n",
    "*baseline 2x4, 4x6, 2x4 -> 2728, 2940, 3030, **2821**, 2440, 2911, 2914, 2924, 3134, 2708, 2534,\n",
    "                           2910, 3352, 2870, 2790, *2816*, 2735, 2763, 2767, 2942, 3030,\n",
    "                           2786, 2942, 2772, 3089, 2777, 2854, 2690, 2912, 2659, 2799,\n",
    "                           2770, 2801, 3021, 2852, 2725, **2825**, 3249, 2635, 3211 *\n",
    "    \n",
    "Alternatives:\n",
    "2x4, 5x5, 3x3      -> 2809, 3087, 3104, 2887, 2767, 2852, 2763, 3067, 2994, 3153, 3011,\n",
    "                      **2929**, 2777, 2745, 2806, 3070, 3197\n",
    "2x4, 4x6, 3x3      -> 3163, 2705, 2931, **2829**, 2915, 3111, 2765, 2791, 3152, 2813, 2797,\n",
    "                      2771, *2836*, 2876, 3039, 2951, 2726, 3144, 2734, 2699, 2732,\n",
    "                      2968, 2755, 3082, 2881, 2723, 2809, 3320, 2659, 2936, *2814*\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
